{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585a8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '.venv/lib/python3.12/site-packages/langchain/indexes': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -R .venv/lib/python3.12/site-packages/langchain/indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a602b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY ì§€ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY ì§€ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b892e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "#print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dbb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader('https://www.langchain.com/')\n",
    "docs = loader.load()\n",
    "\n",
    "#print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb03fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "#print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc04f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "#print(splitted_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e23449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = '''\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\") \n",
    "    \n",
    "# Call the function\n",
    "hello_world()\n",
    "'''\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "\n",
    "#print(python_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b711275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "markdown_text = ''' \n",
    "# ğŸ¦œğŸ”— LangChain âš¡ Building applications with LLMs through composability âš¡ \n",
    "\n",
    "## Quick Install\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open source project in a rapidly developing field, we are extremely open\n",
    "    to contributions.\n",
    "'''\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text], [{'source': 'https://www.langchain.com'}])\n",
    "\n",
    "#print(md_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee64cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "embeddings = model.embed_documents([\n",
    "    'Hi there!',\n",
    "    'Oh, hello!',\n",
    "    'What\\'s your name?',\n",
    "    'My friends call me World',\n",
    "    'Hello World!'\n",
    "])\n",
    "\n",
    "#print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2aff60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "doc = loader.load()\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(doc)\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "\n",
    "#print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "\n",
    "# ë„ì»¤ ì—°ê²° ì„¤ì •\n",
    "connection = 'postgresql+psycopg://postgres:langchain@localhost:6024/langchain'\n",
    "\n",
    "# ë¬¸ì„œë¥¼ ë¡œë“œ í›„ ë¶„í• \n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents, embeddings_model, connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b26e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search('query', k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6b27b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1c623248-9dd4-4b78-b133-6cb4e29d4211',\n",
       " '94ce9fb8-09bc-45e7-9bcb-c0923daf4d73']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥')\n",
    "ids = [str(uuid.uuid4()), str(uuid.uuid4())]\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content='there are cats in the pond',\n",
    "            metadata={'location': 'pond', 'topic': 'animals'},\n",
    "        ),\n",
    "        Document(\n",
    "            page_content='ducks are also found in the pond',\n",
    "            metadata={'location': 'pond', 'topic': 'animals'},\n",
    "        ),\n",
    "    ],\n",
    "    ids=ids,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d36dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idë¡œ ë¬¸ì„œ ì‚­ì œ\n",
      "   94ce9fb8-09bc-45e7-9bcb-c0923daf4d73\n"
     ]
    }
   ],
   "source": [
    "print('idë¡œ ë¬¸ì„œ ì‚­ì œ\\n  ', ids[1])\n",
    "db.delete({'ids': ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0ae63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ë±ì‹± 1íšŒì°¨ ì™„ë£Œ\n",
      "ì¸ë±ì‹± 2íšŒì°¨ ì™„ë£Œ (ì¤‘ë³µ ì—†ìŒ)\n",
      "ì¸ë±ì‹± 3íšŒì°¨ ì™„ë£Œ (ë¬¸ì„œ ì—…ë°ì´íŠ¸)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import List\n",
    "\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. ì„¤ì •\n",
    "# =========================\n",
    "CONNECTION = \"postgresql+psycopg://postgres:langchain@localhost:6024/langchain\"\n",
    "COLLECTION_NAME = \"my_docs\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. VectorStore ìƒì„±\n",
    "# =========================\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,   # âœ… í•µì‹¬ ìˆ˜ì •\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. ë¬¸ì„œ ID ìƒì„±\n",
    "# =========================\n",
    "def make_doc_id(doc: Document) -> str:\n",
    "    raw = f\"{doc.metadata['source']}::{doc.page_content}\"\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. ì¦ë¶„ ì¸ë±ì‹±\n",
    "# =========================\n",
    "def incremental_index(docs: List[Document]) -> None:\n",
    "    for doc in docs:\n",
    "        source = doc.metadata[\"source\"]\n",
    "\n",
    "        # ê°™ì€ source ë¬¸ì„œ ì œê±°\n",
    "        vectorstore.delete(\n",
    "            filter={\"source\": source}\n",
    "        )\n",
    "\n",
    "        # ìƒˆ ë¬¸ì„œ ì‚½ì…\n",
    "        vectorstore.add_documents(\n",
    "            documents=[doc],\n",
    "            ids=[make_doc_id(doc)],\n",
    "        )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. ë¬¸ì„œ ìƒì„±\n",
    "# =========================\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"there are cats in the pond\",\n",
    "        metadata={\"id\": 1, \"source\": \"cats.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ducks are also found in the pond\",\n",
    "        metadata={\"id\": 2, \"source\": \"ducks.txt\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. ì‹¤í–‰\n",
    "# =========================\n",
    "incremental_index(docs)\n",
    "print(\"ì¸ë±ì‹± 1íšŒì°¨ ì™„ë£Œ\")\n",
    "\n",
    "incremental_index(docs)\n",
    "print(\"ì¸ë±ì‹± 2íšŒì°¨ ì™„ë£Œ (ì¤‘ë³µ ì—†ìŒ)\")\n",
    "\n",
    "docs[0].page_content = \"I just modified this document!\"\n",
    "incremental_index(docs)\n",
    "print(\"ì¸ë±ì‹± 3íšŒì°¨ ì™„ë£Œ (ë¬¸ì„œ ì—…ë°ì´íŠ¸)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0da52575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of loaded docs: 45353\n",
      "summary preview:\n",
      "Chapter 5 explores the rich philosophical and scientific landscape of Ancient Greece, which laid the groundwork for Western thought. It highlights the contributions of Pre-Socratic philosophers like Thales, Anaximander, Heraclitus, and Parmenides, who examined cosmology and the nature of existence. Socrates shifted the focus to ethics and epistemology, introducing the Socratic Method to foster critical thinking about morality and virtue. His ideas were further developed by his student Plato, who established the Academy and created a comprehensive philosophical framework. The chapter emphasizes the lasting influence of these Greek intellectual pursuits on future generations.\n",
      "retrieved doc length: 1848\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "# =========================\n",
    "CONNECTION = \"postgresql+psycopg://postgres:langchain@localhost:6024/langchain\"\n",
    "COLLECTION_NAME = \"summaries\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. ë¬¸ì„œ ë¡œë“œ\n",
    "# =========================\n",
    "loader = TextLoader(\"test.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"length of loaded docs:\", len(docs[0].page_content))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. ë¬¸ì„œ ë¶„í• \n",
    "# =========================\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. ìš”ì•½ ì²´ì¸\n",
    "# =========================\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"ë‹¤ìŒ ë¬¸ì„œë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\\n\\n{doc}\"\n",
    ")\n",
    "\n",
    "summarize_chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = summarize_chain.batch(\n",
    "    chunks,\n",
    "    {\"max_concurrency\": 5},\n",
    ")\n",
    "\n",
    "assert len(summaries) == len(chunks)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. VectorStore (ìš”ì•½ ì €ì¥)\n",
    "# =========================\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. ì›ë³¸ ë¬¸ì„œ ì €ì¥ì†Œ (dict ê¸°ë°˜)\n",
    "# =========================\n",
    "docstore = {}   # â† í•µì‹¬ ë³€ê²½ì \n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. ë¬¸ì„œ ID ìƒì„±\n",
    "# =========================\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8. ìš”ì•½ ë¬¸ì„œ ìƒì„± ë° ì¸ë±ì‹±\n",
    "# =========================\n",
    "summary_docs = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    doc_id = doc_ids[i]\n",
    "\n",
    "    summary_docs.append(\n",
    "        Document(\n",
    "            page_content=summaries[i],\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_index\": i,\n",
    "                \"source\": chunks[i].metadata.get(\"source\", \"unknown\"),\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ì›ë³¸ ë¬¸ì„œ ì €ì¥\n",
    "    docstore[doc_id] = chunks[i]\n",
    "\n",
    "vectorstore.add_documents(summary_docs)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9. ìš”ì•½ ê¸°ë°˜ ê²€ìƒ‰\n",
    "# =========================\n",
    "summary_hits = vectorstore.similarity_search(\n",
    "    \"chapter on philosophy\",\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "print(\"summary preview:\")\n",
    "print(summary_hits[0].page_content)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 10. ì›ë³¸ ë¬¸ì„œ ë³µì›\n",
    "# =========================\n",
    "retrieved_docs = [\n",
    "    docstore[hit.metadata[\"doc_id\"]]\n",
    "    for hit in summary_hits\n",
    "]\n",
    "\n",
    "print(\"retrieved doc length:\", len(retrieved_docs[0].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46cf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61025/1360420173.py:1: UserWarning: \n",
      "********************************************************************************\n",
      "RAGatouille WARNING: Future Release Notice\n",
      "--------------------------------------------\n",
      "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
      "instead of the current Stanford ColBERT backend.\n",
      "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
      "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
      "********************************************************************************\n",
      "  from ragatouille import RAGPretrainedModel\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.retrievers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. RAGatouille ëª¨ë¸ ë¡œë“œ\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/chatbot/.venv/lib/python3.12/site-packages/ragatouille/__init__.py:21\u001b[39m\n\u001b[32m     14\u001b[39m warnings.warn(\n\u001b[32m     15\u001b[39m     _FUTURE_MIGRATION_WARNING_MESSAGE,\n\u001b[32m     16\u001b[39m     \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m     17\u001b[39m     stacklevel=\u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Ensures the warning points to the user's import line\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.0.9post2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGPretrainedModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPretrainedModel\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mRAGTrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGTrainer\n\u001b[32m     24\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mRAGPretrainedModel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRAGTrainer\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/chatbot/.venv/lib/python3.12/site-packages/ragatouille/RAGPretrainedModel.py:5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Literal, Optional, Tuple, TypeVar, Union\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_compressors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDocumentCompressor\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseRetriever\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragatouille\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CorpusProcessor\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.retrievers'"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "import requests\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. RAGatouille ëª¨ë¸ ë¡œë“œ\n",
    "# =========================\n",
    "RAG = RAGPretrainedModel.from_pretrained(\n",
    "    \"colbert-ir/colbertv2.0\"\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. ìœ„í‚¤ë°±ê³¼ í˜ì´ì§€ ë¡œë“œ í•¨ìˆ˜\n",
    "# =========================\n",
    "def get_wikipedia_page(title: str) -> str | None:\n",
    "    \"\"\"\n",
    "    ìœ„í‚¤ë°±ê³¼ í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
    "    \"\"\"\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"RAGatouille_tutorial/0.0.1\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "\n",
    "    return page.get(\"extract\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. ë¬¸ì„œ ë¡œë“œ\n",
    "# =========================\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "if not full_document:\n",
    "    raise RuntimeError(\"Wikipedia page could not be loaded\")\n",
    "\n",
    "print(\"Document length:\", len(full_document))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. ColBERT ì¸ë±ìŠ¤ ìƒì„±\n",
    "# =========================\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")\n",
    "\n",
    "print(\"Indexing completed\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. ê²€ìƒ‰\n",
    "# =========================\n",
    "query = \"What animation studio did Miyazaki found?\"\n",
    "\n",
    "results = RAG.search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(r[\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accident-response-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
