import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from pathlib import Path

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ ì½ê¸°)
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def run_ingestion():
    # 2. íŒŒì¼ ê²½ë¡œ ì„¤ì • (data í´ë” ëª…ì‹œ)
    # í˜„ì¬ íŒŒì¼(ragTest.py)ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent.parent

    file_path = project_root / "data" / "P02_01_01_001_20210101.pdf"

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        print(f"âŒ ì—ëŸ¬: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í”„ë¡œì íŠ¸ ë‚´ì— 'data' í´ë”ë¥¼ ë§Œë“¤ê³  PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return
    
    print(f"âœ… '{file_path}' íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")

    # 3. PDF ë¬¸ì„œ ë¡œë“œ ë° íŒŒì‹±
    loader = PyMuPDFLoader(file_path)
    pages = loader.load()

    print(f"âœ… ë¡œë“œ ì™„ë£Œ: ì´ {len(pages)} í˜ì´ì§€")

    # 4. ì²­í‚¹(Chunking) - ì¡°í•­ ë‹¨ìœ„ ë¬¸ë§¥ ë³´ì¡´ì„ ìœ„í•´ ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        add_start_index=True
    )

    chunks = text_splitter.split_documents(pages)
    print(f"âœ… ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)} ì²­í¬ ìƒì„±")

    # 5. ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥
    print("ì„ë² ë”© ìƒì„± ë° FAISS ë²¡í„° DB ì €ì¥ ì¤‘...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    vector_db = FAISS.from_documents(chunks, embeddings)

    # 6. ë¡œì»¬ì— ì €ì¥
    save_path = project_root / "vectorDB" / "faiss_index_samsung_fire"
    vector_db.save_local(save_path)

    print("-" * 30)
    print(f"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“‚ ë²¡í„° DB ì €ì¥ ìœ„ì¹˜: {os.path.abspath(save_path)}")
    print("-" * 30)

    # 7. ë‚´ìš© í™•ì¸í•˜ê¸°
    vector_db = FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    print(f"ì „ì²´ ë²¡í„° ê°œìˆ˜: {vector_db.index.ntotal}")

    # 8. ì €ì¥ëœ ì›ë³¸ ë¬¸ì„œ ë‚´ìš© í™•ì¸
    # íŒŒì¼ì´ ì»¤ì„œ ì¼ë¶€ë§Œ í™•ì¸
    docstore_dict = vector_db.docstore._dict
    for i, (key, doc) in enumerate(docstore_dict.items()):
        print(f"\n--- ë¬¸ì„œ {i+1} ---")
        print(f"Content: {doc.page_content[:100]}...")  # ì• 100ìë§Œ ì¶œë ¥
        print(f"Metadata: {doc.metadata}")
        if i >= 5:  # ì²˜ìŒ 5ê°œ ë¬¸ì„œë§Œ ì¶œë ¥
            break

if __name__ == "__main__":
    run_ingestion()