import os
import logging
from pathlib import Path
import urllib.parse

import psycopg2
from dotenv import load_dotenv
from google.cloud import storage
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
# ê¸°ì¡´ FAISS ë¥¼ ì‚¬ìš©í•˜ë˜ êµ¬ì¡°ì—ì„œ PGVectorStore ë¥¼ ì‚¬ìš©í•˜ë˜ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
#from langchain_community.vectorstores import FAISS
from langchain_postgres import PGVector, PGEngine
from sqlalchemy import create_engine

# --- 1. ë¡œê¹… ì„¤ì • (ê°œì„ ëœ ë¶€ë¶„) ---
# print() ëŒ€ì‹  í‘œì¤€ ë¡œê¹… ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì˜ ë ˆë²¨ ê´€ë¦¬ì™€ í¬ë§·íŒ…ì„ ì²´ê³„í™”í•©ë‹ˆë‹¤.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_env_config():
    """í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³  í•„ìˆ˜ ì„¤ì •ì„ í™•ì¸"""
    load_dotenv()
    required_vars = ["OPENAI_API_KEY", "DB_HOST", "DB_NAME", "DB_USER", "DB_PASSWORD"]
    for var in required_vars:
        if not os.getenv(var):
            logging.error(f"{var}ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•ŠìŒ.")
            raise ValueError(f"í•„ìˆ˜ ì„¤ì • ëˆ„ë½: {var}")
    
    # ë¹„ë°€ë²ˆí˜¸ íŠ¹ìˆ˜ë¬¸ì(@) ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¸ì½”ë”©
    user = os.getenv("DB_USER")
    password = os.getenv("DB_PASSWORD")
    host = os.getenv("DB_HOST")
    port = os.getenv("DB_PORT", "5432")
    db_name = os.getenv("DB_NAME")
        
    # SQLAlchemy ìŠ¤íƒ€ì¼ ì—°ê²° ë¬¸ìì—´ ìƒì„± (PGVectorStoreìš©)
    conn_str = f"postgresql+psycopg://{user}:{password}@{host}:{port}/{db_name}"

    return conn_str

# --- 2. DB ë° GCS ì—°ë™ í•¨ìˆ˜ 
def get_pending_files():
    """DBì—ì„œ ì•„ì§ ë²¡í„°í™” ë˜ì§€ ì•Šì€(is_vectorized = false) íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜´"""

    raw_password = urllib.parse.unquote(os.getenv("DB_PASSWORD"))

    # psycopg2ëŠ” ë³„ë„ ì¸ì½”ë”© ì—†ì´ ë¹„ë°€ë²ˆí˜¸ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_NAME"),
        user=os.getenv("DB_USER"),
        password=raw_password,
        port=os.getenv("DB_PORT", "5432")
    )

    cur = conn.cursor()
    cur.execute("SELECT file_name FROM pdf_documents WHERE is_vectorized = FALSE")
    files = [row[0] for row in cur.fetchall()]
    cur.close()
    conn.close()

    return files

def update_db_status(file_name, status="completed"):
    """ì²˜ë¦¬ê°€ ì™„ë£Œëœ íŒŒì¼ì˜ ìƒíƒœë¥¼ DBì— ì—…ë°ì´íŠ¸."""

    raw_password = urllib.parse.unquote(os.getenv("DB_PASSWORD"))

    conn = psycopg2.connect(
        host=os.getenv("DB_HOST"),
        database=os.getenv("DB_NAME"),
        user=os.getenv("DB_USER"),
        password=raw_password,
        port=os.getenv("DB_PORT", "5432")
    )

    cur = conn.cursor()
    cur.execute(
        "UPDATE pdf_documents SET is_vectorized = TRUE, status = %s WHERE file_name = %s",
        (status, file_name)
    )

    conn.commit()
    cur.close()
    conn.close()

def process_file(file_name, bucket_name, vector_store):
    """GCSì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„° ì €ì¥ì„ ìˆ˜í–‰"""
    local_path = Path(f"./temp_{file_name}")

    try:
        key_path = os.getenv("GCS_KEY_PATH")
        if not key_path:
            raise ValueError("GCS_KEY_PATH ê°€ .envì— ì„¤ì •ë˜ì§€ ì•ŠìŒ")

        # GCS ë‹¤ìš´ë¡œë“œ
        storage_client = storage.Client.from_service_account_json(key_path)
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)
        blob.download_to_filename(str(local_path))
        logging.info(f"ğŸ“¥ '{file_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
        loader = PyMuPDFLoader(str(local_path))
        pages = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            add_start_index=True
        )

        chunks = text_splitter.split_documents(pages)

        # ë©”íƒ€ë°ì´í„° ì£¼ì… (ì¶œì²˜ ì¶”ì ìš©)
        for chunk in chunks:
            chunk.metadata["source"] = file_name

        # PGVectorStore ì €ì¥
        vector_store.add_documents(chunks)
        logging.info(f"âœ¨ '{file_name}' ë²¡í„° DB ì£¼ì… ì™„ë£Œ ({len(chunks)} ì²­í¬)")

        return True

    except Exception as e:
        logging.error(f"âŒ '{file_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    finally:
        if local_path.exists():
            local_path.unlink()

# --- 4. ë©”ì¸ ì‹¤í–‰ êµ¬ì¡° ---
def main():
    try:
        connection_string = load_env_config()
        pending_files = get_pending_files()

        if not pending_files:
            logging.info("ğŸ’¡ ì²˜ë¦¬í•  ìƒˆë¡œìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        logging.info(f"ğŸš€ ì´ {len(pending_files)}ê°œì˜ íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        async_engine = create_engine(connection_string.replace("postgresql+psycopg", "postgresql+psycopg2"))

        # PGVectorStore ì´ˆê¸°í™”
        vector_store = PGVector(
            connection=async_engine,
            embeddings=OpenAIEmbeddings(model="text-embedding-3-small"),
            collection_name="accident_vectors",
            use_jsonb=True
        )

        # ë²„í‚·ëª… ì„¤ì • 
        bucket_name = "pdf-storage-2026"

        for file_name in pending_files:
            if process_file(file_name, bucket_name, vector_store):
                update_db_status(file_name)
                logging.info(f"âœ… DB ìƒíƒœ ê°±ì‹  ì™„ë£Œ: {file_name}")
        
        logging.info("-" * 30)
        logging.info("ğŸ‰ ëª¨ë“  ë²¡í„°í™” ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚¬ìŠµë‹ˆë‹¤!")
        logging.info("-" * 30)

    except Exception as e:
        logging.error(f"âš ï¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¤‘ë‹¨ë¨: {e}")

if __name__ == "__main__":
    main()