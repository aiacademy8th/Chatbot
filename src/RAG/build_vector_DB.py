import os
import argparse
import logging
from pathlib import Path

from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# --- 1. ë¡œê¹… ì„¤ì • (ê°œì„ ëœ ë¶€ë¶„) ---
# print() ëŒ€ì‹  í‘œì¤€ ë¡œê¹… ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ì˜ ë ˆë²¨ ê´€ë¦¬ì™€ í¬ë§·íŒ…ì„ ì²´ê³„í™”í•©ë‹ˆë‹¤.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_api_key():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    return api_key

def load_and_split_documents(file_path: Path) -> list:
    """PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    if not file_path.exists():
        logging.error(f"'{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise FileNotFoundError(f"ì§€ì •ëœ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    logging.info(f"'{file_path}' íŒŒì¼ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    loader = PyMuPDFLoader(str(file_path))
    pages = loader.load()
    logging.info(f"ë¡œë“œ ì™„ë£Œ: ì´ {len(pages)} í˜ì´ì§€")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        add_start_index=True
    )
    chunks = text_splitter.split_documents(pages)
    logging.info(f"ì²­í‚¹ ì™„ë£Œ: ì´ {len(chunks)} ì²­í¬ ìƒì„±")
    return chunks

def create_and_save_vector_db(chunks: list, save_path: Path):
    """ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë²¡í„° DBë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    logging.info("ì„ë² ë”© ìƒì„± ë° FAISS ë²¡í„° DB ì €ì¥ ì¤‘...")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    # ì €ì¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
    save_path.parent.mkdir(parents=True, exist_ok=True)
    vector_db.save_local(str(save_path))
    logging.info(f"ë²¡í„° DBë¥¼ '{save_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    return embeddings, save_path

def verify_db(save_path: Path, embeddings):
    """ì €ì¥ëœ ë²¡í„° DBë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""
    logging.info("ì €ì¥ëœ ë²¡í„° DB ê²€ì¦ ì‹œì‘...")
    vector_db = FAISS.load_local(str(save_path), embeddings, allow_dangerous_deserialization=True)
    logging.info(f"ì „ì²´ ë²¡í„° ê°œìˆ˜: {vector_db.index.ntotal}")
    
    # ì¼ë¶€ ë¬¸ì„œ ë‚´ìš© í™•ì¸
    try:
        docstore_dict = vector_db.docstore._dict
        logging.info("--- ì €ì¥ëœ ë¬¸ì„œ ìƒ˜í”Œ (ìƒìœ„ 5ê°œ) ---")
        for i, (key, doc) in enumerate(docstore_dict.items()):
            logging.info(f"ë¬¸ì„œ {i+1}: {doc.page_content[:100]}...")
            if i >= 4:
                break
    except Exception as e:
        logging.warning(f"ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- 2. ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬ (ê°œì„ ëœ ë¶€ë¶„) ---
# argparseë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ê²½ë¡œë¥¼ í•˜ë“œì½”ë”©í•˜ëŠ” ëŒ€ì‹ ,
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ ë™ì ìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì¬ì‚¬ìš©ì„±ì„ ë†’ì…ë‹ˆë‹¤.
def parse_arguments():
    """ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description="PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    
    # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    project_root = Path(__file__).resolve().parent.parent.parent
    default_input = project_root / "data" / "P02_01_01_001_20210101.pdf"
    default_output = project_root / "vectorDB" / "faiss_index_samsung_fire"

    parser.add_argument(
        "--input",
        type=str,
        default=str(default_input),
        help="ì²˜ë¦¬í•  PDF íŒŒì¼ì˜ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(default_output),
        help="ìƒì„±ëœ ë²¡í„° DBë¥¼ ì €ì¥í•  ê²½ë¡œ"
    )
    return parser.parse_args()

# --- 3. ë©”ì¸ ë¡œì§ êµ¬ì¡°í™” (ê°œì„ ëœ ë¶€ë¶„) ---
# ê° ê¸°ëŠ¥(API í‚¤ ë¡œë“œ, ë¬¸ì„œ ì²˜ë¦¬, DB ìƒì„±, ê²€ì¦)ì„ ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬
# ì½”ë“œì˜ ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    input_path = Path(args.input)
    output_path = Path(args.output)

    try:
        load_api_key()
        chunks = load_and_split_documents(input_path)
        embeddings, saved_path = create_and_save_vector_db(chunks, output_path)
        verify_db(saved_path, embeddings)
        
        logging.info("-" * 30)
        logging.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logging.info(f"ğŸ“‚ ë²¡í„° DB ì €ì¥ ìœ„ì¹˜: {saved_path.resolve()}")
        logging.info("-" * 30)

    except (ValueError, FileNotFoundError) as e:
        logging.error(f"ì‘ì—… ì‹¤íŒ¨: {e}")
    except Exception as e:
        logging.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main()